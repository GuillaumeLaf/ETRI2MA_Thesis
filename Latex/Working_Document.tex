\documentclass{article}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{geometry}
\usepackage{amsfonts}
\usepackage{xcolor}
\usepackage{cancel} % strikethrough text

\geometry{a4paper, portrait, margin=1cm}

\numberwithin{equation}{section}

\newcommand{\mb}[1]{\boldsymbol{#1}}

\let \oldsum \sum
\renewcommand{\sum}{\displaystyle \oldsum}

\newcommand{\abs}[1]{\left \lvert #1 \right \rvert}

\newcommand{\X}[1]{\bm{X_{#1;T}}}
\newcommand{\eX}[2]{X_{#2;T}^{(#1)}}
\newcommand{\W}[2]{\bm{W}_{#1}\left(#2/T\right)}
\newcommand{\eW}[4]{W_{#3}^{(#1,#2)}\left(#4/T\right)}
\newcommand{\CEWS}[2]{\bm{S}_{#1}\left(#2/T\right)}
\newcommand{\eCEWS}[4]{S_{#3}^{(#1,#2)}\left(#4/T\right)}
\newcommand{\wavelet}[3]{\psi_{#1,#2}(#3)}
\newcommand{\ACwavelet}[2]{\bm{\Psi}_{#1}(#2)}
\newcommand{\increment}[2]{\bm{\xi}_{#1,#2}}
\newcommand{\eincrement}[3]{\xi_{#2,#3}^{(#1)}}

\newcommand{\scalesum}{\sum_{j=-J}^{-1}}
\newcommand{\scalesumi}{\sum_{l=-J}^{-1}}
\newcommand{\locsum}{\sum_{k=0}^{T}}
\newcommand{\locsumi}{\sum_{m=0}^{T}}

\newcommand{\kronecker}[2]{\delta_{#1,#2}}

\newcommand{\coeffs}[2]{\bm{d}_{#1,#2}}
\newcommand{\rawPeriodo}[2]{\bm{I}_{#1,#2}}
\newcommand{\correctedPeriodo}[2]{\bar{\bm{I}}_{#1,#2}}
\newcommand{\smoothPeriodo}[2]{\widetilde{\bm{I}}_{#1,#2}}

\newcommand{\estimCEWS}[2]{\widehat{\bm{S}}_{#1}\left(#2/T\right)}
\newcommand{\estimLoadings}[2]{\widehat{\bm{\Lambda}}_{#1,#2}}
\newcommand{\estimFactors}[1]{\widehat{\bm{F}}_{#1}}

\newcommand{\E}[1]{\mathrm{E}\left[#1\right]}
\newcommand{\Var}[1]{\mathrm{Var}\left[#1\right]}
\newcommand{\Cov}[2]{\mathrm{Cov}\left[#1,#2\right]}

\newcommand{\loadings}[2]{\bm{\Lambda}_{#1,#2}}
\newcommand{\optLoadings}[2]{\bar{\bm{\Lambda}}_{#1,#2}}
\newcommand{\setOptLoadings}{\{ \bar{\bm{\Lambda}}_{j,k} \}_{\forall j,k}}
\newcommand{\factors}[1]{\bm{F}_{#1}}
\newcommand{\optFactors}[1]{\bar{\bm{F}}_{#1}}
\newcommand{\setOptFactors}{\{ \bar{\bm{F}}_{k}\}_{\forall j,k}}
\newcommand{\idioError}[2]{\bm{\epsilon}_{#1,#2}}

\begin{document}
\section{Quantities}
	\begin{itemize}
		\item $J \in \mathbb{Z}^{+}$ = number of scales decomposition
		\item $T = 2^{J}$ = number of time periods 
		\item $N \in \mathbb{Z}^{+}$ = number of cross-section elements
		\item $K (\leq N)$ = number of common factors
	\end{itemize}

\section{Multivariate Locally Stationary Wavelet process (Park et al. (2014))}
	The vector $(N \times 1)$ of stochastic processes $\X{t}$ follows the given decomposition : 
	\begin{align}\label{eq:lswStructure}
		 \X{t} = \scalesum \locsum \W{j}{k} \increment{j}{k} \wavelet{j}{k}{t} 
	\end{align}
	where 
	\begin{itemize}
		\item $\bm{W_{j}(z)}$ is a lower-triangular $(N \times N)$ matrix. \\ 
			For each $(m,n)$-element, 
			\begin{align}
				 &W_{j}^{(m,n)}(z) \text{ is a Lipschitz continuous function on } z \in (0,1) \\
				 &\sum_{j=-\infty}^{-1} \abs{W_{j}^{(m,n)}(z)}^{2} < \infty, \qquad \forall z \in (0,1) \hspace{1cm} \text{(finite energy)}\\
				 &\sum_{j=-\infty}^{-1} 2^{-j}L_{j}^{(m,n)} < \infty  \hspace{2cm} \text{(uniformly bounded Lipschitz constants $L_{j}$)}
			\end{align}
		\item $\increment{j}{k}$ is the vector $(N \times 1)$ of random orthonormal increments.
			\begin{align}
				\E{\eincrement{u}{j}{k}} &= 0, \qquad &\forall j,k,u \\ \label{eq:indepIncrements}
				\Cov{\eincrement{u}{j}{k}}{\eincrement{u'}{j'}{k'}} &= \kronecker{j}{j'}\kronecker{k}{k'}\kronecker{u}{u'}, \qquad &\forall j,j',k,k',u,u' 
			\end{align}

		\item $\wavelet{j}{k}{t} = \psi_{j,k-t}$ is a scalar representing a non-decimated wavelet.
	\end{itemize}

We can define the \emph{Cross-Evoluationary Wavelet Spectrum} $(N \times N)$ matrix : $\bm{S}_{j}(z) = \bm{W}_{j}(z)\bm{W}_{j}(z)'$.
This gives us the ability to express the \emph{local autocovariance} : $c^{(u,u')}(z,\tau) = \sum_{j=-\infty}^{-1} S_{j}^{(u,u')}(z) \ACwavelet{j}{\tau}$ where $\ACwavelet{j}{\tau} = \sum_{k} \wavelet{j}{k}{0}\wavelet{j}{k}{\tau}$, the \emph{autocorrelation wavelet}. The latter also define the \emph{inner product matrix of discrete autocorrelation wavelets} : $A_{jl} = \sum_{\tau}\ACwavelet{j}{\tau}\ACwavelet{l}{\tau}$, $A = \left\{ A_{jl}\right\}_{j,l \in \mathbb{N}}$ and its inverse : $\bar{A} = A^{-1}$.\\

Each $(m,n)$-element of the Cross-Evolutionary Wavelet Spectrum can be expressed as 
\begin{align*}
	\eCEWS{m}{n}{j}{k} = \sum_{u=1}^{N} \eW{m}{u}{j}{k}\eW{u}{n}{j}{k}, \quad \forall j,k
\end{align*}
From this definition it is not difficult to extend the CEWS to take into account the dependence structure between different scales and throught time : 
\begin{align}\label{eq:serial-scale-CEWS}
	S_{j,j'}^{m,n}\left( k/T, k'/T \right) = \sum_{u=1}^{N} \eW{m}{u}{j}{k}\eW{u}{n}{j'}{k'}, \quad \forall j,j',k,k'
\end{align}
We make the following assumption regarding the latter object, 
\begin{align}\label{eq:scale-indep-spectrum}
S_{j,j'}^{(m,n)}\left( \textcolor{blue}{k/T}, \textcolor{blue}{k/T} \right) = 
	\begin{cases}
		\eCEWS{m}{n}{j}{k} & \text{ if } j=j' \\
		0 & \text{ otherwise}
	\end{cases}
\end{align}
This assumption \textcolor{orange}{(possible improvement : condition similar to Chamberlain ?)}  imposes no dependence between different scales of decomposition. Notice that we don't restrict the serial dependence. 

\subsection{Estimation of MvLSW}
\begin{itemize}
	\item $\coeffs{j}{k} = \sum_{t=0}^{T-1}\bm{X}_t \wavelet{j}{k}{t}$  \hspace{2cm} (empirical wavelet coefficients)
	\item $\rawPeriodo{j}{k} = \coeffs{j}{k}\coeffs{j}{k}'$  \hspace{2cm} (raw wavelet periodogram)
		\begin{itemize}
			\item $\E{\rawPeriodo{j}{k}} = \sum_{l=-J}^{-1} A_{jl}\CEWS{l}{k} + O(T^{-1})$ \hspace{2cm} (biaised estimator)
		\end{itemize}
	\item $\correctedPeriodo{j}{k} = \sum_{l=-J}^{-1} \bar{A}_{j,l}\rawPeriodo{l}{k}$ \hspace{2cm} (corrected periodogram) $\Longrightarrow$ (unbiased estimator)
	\item $\smoothPeriodo{j}{k} = \frac{\displaystyle 1}{\displaystyle 2M+1} \sum_{m=-M}^{M}\rawPeriodo{j}{k+m} $ \hspace{2cm} (smooth periodogram) $\Longrightarrow$ (consistent estimator)
	\item $\estimCEWS{j}{k} = \sum_{l=-J}^{-1}\bar{A}_{jl}\smoothPeriodo{l}{k} = \frac{\displaystyle 1}{\displaystyle 2M+1} \sum_{m=-M}^{M}\correctedPeriodo{j}{k+m} = \frac{\displaystyle 1}{\displaystyle 2M+1} \sum_{m=-M}^{M}\sum_{l=-J}^{-1} \bar{A}_{j,l}\rawPeriodo{l}{k+m}$ \hspace{1cm} (final estimator of CEWS)
\end{itemize}

\subsection{Notes}
	\begin{itemize}
		\item The dependence structure is entirely in $\bm{W}_{j}(z)$, not in $\increment{j}{k}$.
		\item The lower-triangular form of $\bm{W}_{j}(z)$ allows us to use the Cholesky decomposition on $\bm{S}_{j}(z)$.
	\end{itemize}

\section{Factor Model}
	\begin{itemize}
		\item The factor structure is imposed on the following : 
			\begin{align} \label{eq:factorStructure}
				\W{j}{k} \increment{j}{k} = \loadings{j}{k} \factors{k} + \idioError{j}{k}
			\end{align}
			, not only on $\increment{j}{k}$ since they are assumed orthonormal.
		\item Assumptions : 	
			\begin{enumerate}
				\item $\factors{k} \sim (\bm{0}, \bm{\Sigma}_F)$, where $\bm{\Sigma}_F$ is a diagonal positive definite $(K \times K)$ matrix. 
				\item $\factors{k} \perp \idioError{j}{k'}$, $\forall j,k,k'$
				\item $\idioError{j}{k} \sim (\bm{0}, \bm{\Sigma}_{\epsilon})$, where $\bm{\Sigma}_{\epsilon}$ has bounded eigenvalues. \textcolor{blue}{Note : make $\bm{\Sigma}_{\epsilon}$ dependent on time ? what about serial dependence ?}
				\item  $\loadings{j}{k}'\loadings{l}{m} = \bm{0}, \forall j \neq l, \forall k \neq m$. 
			\end{enumerate}
		\item We can then represent the CEWS with the factor structure : 
		\begin{align*}
			\Var{\W{j}{k}\increment{j}{k}}        	    &= \Var{\loadings{j}{k}\factors{k}} + \Var{\idioError{j}{k}} \\
			\W{j}{k} \Var{\increment{j}{k}} \W{j}{k}' &= \loadings{j}{k}\Var{\factors{k}}\loadings{j}{k}' +  \Var{\idioError{j}{k}}\\
			\W{j}{k}\W{j}{k}' &=  \loadings{j}{k}\bm{\Sigma}_{F}\loadings{j}{k}' +  \bm{\Sigma}_{\epsilon} &\text{ from } \eqref{eq:indepIncrements}\\
			\CEWS{j}{k} &= \loadings{j}{k}\bm{\Sigma}_{F}\loadings{j}{k}' +  \bm{\Sigma}_{\epsilon}
		\end{align*}
	\end{itemize}










%	An important quantity to analyse is : 
%	\begin{align*}
%		\W{j}{k}\increment{j}{k}\increment{j'}{k'}'\W{j'}{k'}' = \loadings{j}{k}\factors{k}\factors{k'}'\loadings{j'}{k'}' + \idioError{j}{k}\factors{k'}'\loadings{j'}{k'}' + \loadings{j}{k}\factors{k}\idioError{j'}{k'}' + \idioError{j}{k}\idioError{j'}{k'}'
%	\end{align*}
%	Each $(m,n)$-element of the matrix on the RHS can be written as : 
%	\begin{align*}
%		\sum_{u}\sum_{u'} \eW{m}{u}{j}{k}\eincrement{u}{j}{k}\eincrement{u'}{j'}{k'}\eW{u'}{n}{j'}{k'} \left(= \sum_{u}\ \eW{m}{u}{j}{k}\eincrement{u}{j}{k} \sum_{u'}\eW{u'}{n}{j'}{k'} \eincrement{u'}{j'}{k'}\right)
%	\end{align*}
%	\footnote{Can analyse this term as $\sum_u  \eW{m}{u}{j}{k}\eincrement{u}{j}{k} \sum_{u'} \eW{u'}{n}{j'}{k'}\eincrement{u'}{j'}{k'}$ and then use Slutsky ?}.
%
%	The expectation of each term is therefore, 
%	\begin{align*}
%		\E{\eW{m}{u}{j}{k}\eincrement{u}{j}{k}\eincrement{u'}{j'}{k'}\eW{u'}{n}{j'}{k'}} &= \eW{m}{u}{j}{k}\E{\eincrement{u}{j}{k}\eincrement{u'}{j'}{k'}}\eW{u'}{n}{j'}{k'}\\
%		&= \begin{cases}
%			\eW{m}{u}{j}{k}\eW{u}{n}{j}{k} \quad &\text{ if } j=j',k=k',u=u'\\
%			0  \quad & \text{otherwise}
%		      \end{cases} \quad \text{form } \eqref{eq:indepIncrements}
%	\end{align*}
%	To apply a WLLN we need finite variances on each term :  
%	\begin{align*}
%		&\Var{\eW{m}{u}{j}{k}\eincrement{u}{j}{k}\eincrement{u'}{j'}{k'}\eW{u'}{n}{j'}{k'}} = \left(\eW{m}{u}{j}{k}\right)^{2}\Var{\eincrement{u}{j}{k}\eincrement{u'}{j'}{k'}}\left(\eW{u'}{n}{j'}{k'}\right)^{2}\\
%		&\Var{\eincrement{u}{j}{k}\eincrement{u'}{j'}{k'}} = 
%			\begin{cases}
%				\E{\left(\eincrement{u}{j}{k}\eincrement{u'}{j'}{k'}\right)^{2}} = \footnotesize \begin{cases}
%															\E{(\eincrement{u}{j}{k})^{2}}\E{(\eincrement{u'}{j'}{k'})^{2}} = 1 &\text{ if } \eincrement{u}{j}{k} \text{ is Gaussian and from}\eqref{eq:indepIncrements}\\ 			
%															\leq \E{(\eincrement{u}{j}{k})^{4}}^{1/2}\E{(\eincrement{u'}{j'}{k'})^{4}}^{1/2}	&\text{ otherwise and from Cauchy. }			
%														\end{cases} &\text{ if } j\neq j', k\neq k', u\neq u' \normalsize\\
%				\Var{(\eincrement{u}{j}{k})^{2}} \leq \E{(\eincrement{u}{j}{k})^{4}} & \text{ otherwise }
%			\end{cases}
%	\end{align*}
%	which suggests that we need finite fourth moment for the increment of the LSW process in order to have convergence in probability. \\
%	If $\E{(\eincrement{u}{j}{k})^{4}}< \infty$, \textcolor{blue}{convergence ? But double sum...}.\\
%	
%	\textcolor{blue}{
%	Each $(m,n)$-element of the matrix on the RHS can be written as : 
%	\begin{align*}
%		\sum_{u}\sum_{u'} \eW{m}{u}{j}{k}\eincrement{u}{j}{k}\eincrement{u'}{j'}{k'}\eW{u'}{n}{j'}{k'} = \sum_{u}\ \eW{m}{u}{j}{k}\eincrement{u}{j}{k} \sum_{u'}\eW{u'}{n}{j'}{k'} \eincrement{u'}{j'}{k'}
%	\end{align*}
%	We can analyse the convergence of the two sum independently and then apply Slutsky. 
%	The expectation of each term in the first sum is therefore, 
%	\begin{align*}
%		\E{\eW{m}{u}{j}{k}\eincrement{u}{j}{k}} &= \eW{m}{u}{j}{k}\E{\eincrement{u}{j}{k}} = 0\\
%	\end{align*}
%	To apply a WLLN we need finite variances on each term :  
%	\begin{align*}
%		\Var{\eW{m}{u}{j}{k}\eincrement{u}{j}{k}} &= \left(\eW{m}{u}{j}{k}\right)^{2}\Var{\eincrement{u}{j}{k}}\\
%								        &= \left(\eW{m}{u}{j}{k}\right)^{2} < \infty \quad &\text{from }\eqref{eq:indepIncrements} \text{ and }\eqref{eq:finiteEnergy}
%	\end{align*}
%	Then, 
%	\begin{align*}
%		\frac{1}{N} \sum_{u=1}^{N} \eW{m}{u}{j}{k}\eincrement{u}{j}{k} \overset{p}{\longrightarrow} 0 \quad \text{when } N \longrightarrow \infty
%	\end{align*}
%	Finally by Slutsky, 
%	\begin{align*}
%		\frac{1}{N}\sum_{u}\ \eW{m}{u}{j}{k}\eincrement{u}{j}{k} \frac{1}{N}\sum_{u'}\eW{u'}{n}{j'}{k'} \eincrement{u'}{j'}{k'} \overset{p}{\longrightarrow} 0
%	\end{align*}
%	\textcolor{blue}{There is a problem... This result would mean that the factor structure is imposed on a null matrix asymptotically. }}


\subsection{Estimation}
	The estimation of the loadings and common factors is carried out by a non-linear least square procedure in the wavelet domain.
	\begin{align}
		\min_{\setOptLoadings,\setOptFactors} \quad & (NT)^{-1} \sum_t \left[\X{t} - \scalesum \locsum \left(\optLoadings{j}{k} \optFactors{k}\right) \wavelet{j}{k}{t}\right]'\left[\X{t} - \scalesum \locsum \left(\optLoadings{j}{k} \optFactors{k}\right) \wavelet{j}{k}{t}\right]\nonumber \\ 
	\textrm{s.t.} \quad & \frac{\optLoadings{j}{k}'\optLoadings{j}{k}}{N} = \bm{I}_K \label{eq:constraint}
	\end{align}
	After distributing the objective function becomes, 
\small
	\begin{align*}
 (NT)^{-1} \sum_t \Big[\X{t}'\X{t} - \X{t}' \scalesum \locsum \optLoadings{j}{k}\optFactors{k}\wavelet{j}{k}{t}
	-\scalesum \locsum \wavelet{j}{k}{t}\optFactors{k}'\optLoadings{j}{k}' \X{t} 
        + \scalesum \locsum \scalesumi \locsumi \wavelet{j}{k}{t}\wavelet{l}{m}{t}\optFactors{k}'\optLoadings{j}{k}'\optLoadings{l}{m}\optFactors{m} \Big]\
	\end{align*}
\normalsize
\footnotesize
	\begin{align*}
	 (NT)^{-1} \Big[ \sum_t \X{t}'\X{t} - \scalesum \locsum \textcolor{blue}{\sum_t  \X{t}'\wavelet{j}{k}{t}} \optLoadings{j}{k}\optFactors{k}
		-\scalesum \locsum \optFactors{k}'\optLoadings{j}{k}' \textcolor{blue}{\sum_t  \X{t} \wavelet{j}{k}{t}}
	        + \sum_t \scalesum \locsum \scalesumi \locsumi \wavelet{j}{k}{t}\wavelet{l}{m}{t}\optFactors{k}'\optLoadings{j}{k}'\optLoadings{l}{m}\optFactors{m} \Big]
	\end{align*}
\normalsize
	By definition of the empirical wavelet coefficents,
\small
	\begin{align*}
	 (NT)^{-1} \Big[ \sum_t \X{t}'\X{t} - \scalesum \locsum \textcolor{blue}{\coeffs{j}{k}'} \optLoadings{j}{k}\optFactors{k}
		-\scalesum \locsum \optFactors{k}'\optLoadings{j}{k}' \textcolor{blue}{\coeffs{j}{k}} 
	       + \sum_t \scalesum \locsum \scalesumi \locsumi \wavelet{j}{k}{t}\wavelet{l}{m}{t}\optFactors{k}'\optLoadings{j}{k}'\optLoadings{l}{m}\optFactors{m} \Big]
	\end{align*}
\normalsize     			      

	By assumption on the loadings \textcolor{orange}{(possible improvement)} and the fact that wavelets are normalized $ \sum_{t}(\wavelet{j}{k}{t})^{2} = 1, \forall j,k$, 
\small
	\begin{align*}
	 (NT)^{-1} \Big[ \sum_t \X{t}'\X{t} - \scalesum \locsum \textcolor{blue}{\coeffs{j}{k}'} \optLoadings{j}{k}\optFactors{k}
		-\scalesum \locsum \optFactors{k}'\optLoadings{j}{k}' \textcolor{blue}{\coeffs{j}{k}} 
	       +  \scalesum \locsum \optFactors{k}'\optLoadings{j}{k}'\optLoadings{j}{k}\optFactors{k} \Big]
	\end{align*}
\normalsize    

The First Order Conditions with respect to the factors are given by : 
	\begin{align}
		\optFactors{k}' \scalesum \optLoadings{j}{k}'\optLoadings{j}{k} - \scalesum \coeffs{j}{k}' \optLoadings{j}{k} &= 0 \quad, \forall k \nonumber\\
		\optFactors{k}' \scalesum N &= \scalesum \coeffs{j}{k}' \optLoadings{j}{k} \quad, \forall k &\text{ from } \eqref{eq:constraint} \nonumber \\
		\optFactors{k}'  &= (JN)^{-1}\scalesum \coeffs{j}{k}' \optLoadings{j}{k} \label{eq:optimalFactors}
	\end{align}

%	The First Order Condition with respect to $\optFactors{k}$ are : \textcolor{blue}{not correct}
%	\begin{align*}
%		\optFactors{k}' \scalesum \scalesumi \sum_{m} \optLoadings{j}{k}' \optLoadings{l}{m} \factors{m} \textcolor{red}{\sum_t \wavelet{j}{k}{t}\wavelet{l}{m}{t}} = \scalesum \coeffs{j}{k}' \optLoadings{j}{k} \quad, \forall k
%	\end{align*}
%	\textcolor{red}{The problem is that we cannot inverse $\sum_t \wavelet{j}{k}{t}\wavelet{l}{m}{t}$ since it could be null and that we define the common factor in term of itself. Solution : show that $\optFactors{k}'\optLoadings{j}{k}'\optLoadings{l}{m}\optFactors{m}$ is (asymptotically) zero when indices differ.}
%	With the constraint \eqref{eq:constraint} and taking the transpose,
 
%	\begin{align}\label{eq:optFactor}
%		\cancel{\optFactors{k} \scalesum \scalesumi \sum_t \wavelet{j}{k}{t}\wavelet{l}{m}{t} = (N)^{-1} \scalesum \optLoadings{j}{k}' \coeffs{j}{k} \quad, \forall k}
%	\end{align}

	Replace \eqref{eq:optimalFactors} in the original minimization problem, 
	\begin{align*}
		\min_{\setOptLoadings} \quad (NT)^{-1} \Big[ \sum_t \X{t}'\X{t} &- \scalesum \locsum\coeffs{j}{k}' \optLoadings{j}{k}\left((JN)^{-1}\scalesumi  \optLoadings{l}{k}' \coeffs{l}{k}\right) \\
		&- \scalesum \locsum \left((JN)^{-1}\scalesumi  \coeffs{l}{k}' \optLoadings{l}{k} \right)\optLoadings{j}{k}' \coeffs{j}{k} \\
	       &+ \scalesum \locsum \left((JN)^{-1}\scalesumi  \coeffs{l}{k}' \optLoadings{l}{k} \right)\optLoadings{j}{k}'\optLoadings{j}{k}\left((JN)^{-1}\sum_{n=-J}^{-1}  \optLoadings{n}{k}' \coeffs{n}{k}\right) \Big]
	\end{align*}

	\begin{align*}
		\min_{\setOptLoadings} \quad (NT)^{-1} \Big[ \sum_t \X{t}'\X{t} &- (JN)^{-1}\scalesumi \scalesum \locsum \coeffs{j}{k}' \optLoadings{j}{k}\optLoadings{l}{k}' \coeffs{l}{k} \\
		&- (JN)^{-1}\scalesumi \scalesum \locsum \coeffs{l}{k}' \optLoadings{l}{k} \optLoadings{j}{k}' \coeffs{j}{k} \\
	       &+ (JN)^{-2}\scalesumi \sum_{n=-J}^{-1} \textcolor{blue}{\scalesum} \locsum \coeffs{l}{k}' \optLoadings{l}{k} \textcolor{blue}{\optLoadings{j}{k}' \optLoadings{j}{k}} \optLoadings{n}{k}' \coeffs{n}{k} \Big]
	\end{align*}

	\begin{align*}
		\min_{\setOptLoadings} \quad (NT)^{-1} \Big[ \sum_t \X{t}'\X{t} &- (JN)^{-1}\scalesumi \scalesum \locsum \coeffs{j}{k}' \optLoadings{j}{k}\optLoadings{l}{k}' \coeffs{l}{k} \\
		&- (JN)^{-1}\scalesumi \scalesum \locsum \coeffs{l}{k}' \optLoadings{l}{k} \optLoadings{j}{k}' \coeffs{j}{k} \\
	       &+ (JN)^{-2} \textcolor{blue}{JN}\scalesumi \sum_{n=-J}^{-1} \scalesum \locsum \coeffs{l}{k}' \optLoadings{l}{k} \optLoadings{n}{k}' \coeffs{n}{k} \Big] 			&\quad \text{from } \eqref{eq:constraint}
	\end{align*}

\begin{align*}
	\min_{\setOptLoadings} \quad (NT)^{-1} \Big[ \sum_t \X{t}'\X{t} &- (JN)^{-1}\scalesumi \scalesum \locsum \coeffs{j}{k}' \optLoadings{j}{k}\optLoadings{l}{k}' \coeffs{l}{k}\Big]
\end{align*}
Minimizing the latter expression is equivalent to maximizing, 
\begin{align*}
	\max_{\setOptLoadings} \quad (JT)^{-1}N^{-2}\scalesumi \scalesum \locsum \coeffs{j}{k}' \optLoadings{j}{k}\optLoadings{l}{k}' \coeffs{l}{k}
\end{align*}
Each term in the triple sum is a scalar, i.e. $(1 \times 1)$ matrix. Therefore we can freely take its trace, 
\begin{align*}
	\max_{\setOptLoadings} \quad (JT)^{-1}N^{-2}\scalesumi \scalesum \locsum \mathrm{tr} \left \{\coeffs{j}{k}' \optLoadings{j}{k}\optLoadings{l}{k}' \coeffs{l}{k}\right \}
\end{align*}
From the cyclic property of the trace, 
\begin{align*}
	&\max_{\setOptLoadings} \quad (JT)^{-1}N^{-2}\scalesumi \scalesum \locsum \mathrm{tr} \left \{\optLoadings{l}{k}' \coeffs{l}{k} \coeffs{j}{k}' \optLoadings{j}{k} \right \}\\
	&\max_{\setOptLoadings} \quad (JT)^{-1}N^{-2} \left[ \locsum \scalesum \mathrm{tr} \left \{\optLoadings{j}{k}' \coeffs{j}{k} \coeffs{j}{k}' \optLoadings{j}{k} \right \} + \textcolor{blue}{2} \locsum \scalesum \sum_{l=-J}^{\textcolor{blue}{j}-1} \mathrm{tr} \left \{\optLoadings{l}{k}' \coeffs{l}{k} \coeffs{j}{k}' \optLoadings{j}{k} \right \} \right]
\end{align*}
We recognize the raw wavelet periodogram $\coeffs{l}{k} \coeffs{j}{k}'$ . We replace the latter with the unbiased and consistent estimator of the CEWS, i.e. $\estimCEWS{j}{k}$. \\
Therefore the first term becomes, 
\begin{align*}
	\locsum \scalesum \mathrm{tr} \left \{\optLoadings{j}{k}' \estimCEWS{j}{k} \optLoadings{j}{k} \right \}
\end{align*}
The second term needs a similar treatement. We replace $\coeffs{l}{k} \coeffs{j}{k}'$ with the unbiaised and consistent estimator of \eqref{eq:serial-scale-CEWS} : 
\begin{align}
	\Hat{\bm{S}}_{j,j'}(k/T, k/T) = \frac{\displaystyle 1}{\displaystyle 2M+1} \sum_{m=-M}^{M}\sum_{r=-J}^{-1} \sum_{l=-J}^{-1} \bar{A}_{j,l}^{(j',r)} \coeffs{j}{k+m} \coeffs{j'}{k+m}'
\end{align}
where $\bar{A}_{j,l}^{(j',l')} = \sum_{\tau}\Psi_{j,j'}(\tau)\Psi_{l,l'}(\tau)$ and $\Psi_{j,j'}(\tau) = \sum_{t} \wavelet{j}{0}{t}\wavelet{j'}{\tau}{t}$, the inner product operator of the cross-correlation wavelet functions and the \emph{Cross-Correlation Wavelet Function}, respectively. \textcolor{red}{In his thesis Koch} showed that this CCWF inherit the same properties as the autocorrelation function. One of the latter is that the functions in that family are linearly indepedent of each other. Consequently, the inner product operator of that family is invertible.  \\

The second term on the maximization problem thus reads, 
\begin{align*}
	2 \locsum \scalesum \sum_{l=-J}^{j-1} \mathrm{tr} \left \{\optLoadings{l}{k}' \Hat{\bm{S}}_{j,l}(k/T, k/T) \optLoadings{j}{k} \right \}
\end{align*}
Consequently, the whole optimization problem is changed into
\begin{align*}
	\max_{\setOptLoadings} \quad (JT)^{-1}N^{-2} \left[ \locsum \scalesum \mathrm{tr} \left \{\optLoadings{j}{k}' \estimCEWS{j}{k} \optLoadings{j}{k} \right \} + 2 \locsum \scalesum \sum_{l=-J}^{j-1} \mathrm{tr} \left \{\optLoadings{l}{k}' \Hat{\bm{S}}_{j,l}(k/T, k/T) \optLoadings{j}{k} \right \} \right]
\end{align*}
which is asymptotically equivalent to (see Park et al. (2014)),
\begin{align*}
	\max_{\setOptLoadings} \quad (JT)^{-1}N^{-2} \left[ \locsum \scalesum \mathrm{tr} \left \{\optLoadings{j}{k}' \CEWS{j}{k} \optLoadings{j}{k} \right \} + 2 \locsum \scalesum \sum_{l=-J}^{j-1} \mathrm{tr} \left \{\optLoadings{l}{k}' \bm{S}_{j,l}(k/T, k/T) \optLoadings{j}{k} \right \} \right]
\end{align*}
The last term is zero by assumption \eqref{eq:scale-indep-spectrum}, 
\begin{align*}
	\max_{\setOptLoadings} \quad (JT)^{-1}N^{-2} \locsum \scalesum \mathrm{tr} \left \{\optLoadings{j}{k}' \CEWS{j}{k} \optLoadings{j}{k} \right \}
\end{align*}
Finally, we get back the feasible problem, 
\begin{align}\label{eq:empiricalOptimization}
	\max_{\setOptLoadings} \quad (JT)^{-1}N^{-2} \locsum \scalesum \mathrm{tr} \left \{\optLoadings{j}{k}' \estimCEWS{j}{k} \optLoadings{j}{k} \right \} + O(M^{-1})
\end{align}
where $M(T) \to \infty$ when $T \to \infty$.

This final problem can be decompose into sub-problems and the latter can be solved independently. In other words, the problem \eqref{eq:empiricalOptimization} is maximized when each term in the double sum is also maximized. \textcolor{red}{(Proof)}
\begin{align*}
	\textcolor{blue}{\max_{\setOptLoadings}} \quad (JT)^{-1} \locsum \scalesum N^{-2} \mathrm{tr} \left \{\optLoadings{j}{k}' \estimCEWS{j}{k} \optLoadings{j}{k} \right \} \quad = \quad (JT)^{-1} \locsum \scalesum \textcolor{blue}{\max_{\optLoadings{j}{k}}} \text{ } N^{-2} \mathrm{tr} \left \{\optLoadings{j}{k}' \estimCEWS{j}{k} \optLoadings{j}{k} \right \}
\end{align*}
This solution of those final optimization problems are $\sqrt{N}\bm{\tilde{\Lambda}}_{j,k}$, where $\bm{\tilde{\Lambda}}_{j,k}$ is the $(N \times K)$ matrix whose columns are the first $K$ orthonormal eigenvectors of $\estimCEWS{j}{k}$.\\

We obtain the wanted least squares estimators of the loadings and factors as 
\begin{align}
	\estimLoadings{j}{k} &= \sqrt{N}\bm{\tilde{\Lambda}}_{j,k} \label{eq:estimatorLoadings}\\
	\estimFactors{k} &= (JN)^{-1} \scalesum \estimLoadings{j}{k}' \coeffs{j}{k} \label{eq:estimatorFactors}
\end{align}




\section{Goals}
\begin{itemize}
	\item $\lVert \hat{\bm{\Lambda}}_{j,k} - \bm{\Lambda}_{j,k}\bm{R}_{j,k} \rVert = O(1)$
	\item $\lVert \hat{\bm{F}}_{k} - \bm{R}^{-1}_{\textcolor{red}{j},k}\bm{F}_{k} \rVert = O(1)$
\end{itemize}

































	
\end{document}