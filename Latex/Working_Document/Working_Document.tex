\documentclass[main_document.tex]{subfiles}

\begin{document}

\section{Quantities}
	\begin{itemize}
		\item $J \in \mathbb{Z}^{+}$ = number of scales decomposition
		\item $T = 2^{J}$ = number of time periods 
		\item $N \in \mathbb{Z}^{+}$ = number of cross-section elements
		\item $K (\leq N)$ = number of common factors
	\end{itemize}

\section{Multivariate Locally Stationary Wavelet process (Park et al. (2014))}
	The vector $(N \times 1)$ of stochastic processes $\X{t}$ follows the given decomposition : 
	\begin{align}\label{eq:lswStructure}
		 \X{t} = \scalesum{j} \locsum{k} \W{j}{k} \increment{j}{k} \wavelet{j}{k}{t} 
	\end{align}
	where 
	\begin{itemize}
		\item $\bm{W_{j}(z)}$ is a lower-triangular $(N \times N)$ matrix. \\ 
			For each $(m,n)$-element, 
			\begin{align}
				 &W_{j}^{(m,n)}(z) \text{ is a Lipschitz continuous function on } z \in (0,1) \\
				 &\sum_{j=-\infty}^{-1} \abs{W_{j}^{(m,n)}(z)}^{2} < \infty, \qquad \forall z \in (0,1) \hspace{1cm} \text{(finite energy)}\\
				 &\sum_{j=-\infty}^{-1} 2^{-j}L_{j}^{(m,n)} < \infty  \hspace{2cm} \text{(uniformly bounded Lipschitz constants $L_{j}$)}
			\end{align}
		\item $\increment{j}{k}$ is the vector $(N \times 1)$ of random orthonormal increments.
			\begin{align}
				\E{\eincrement{u}{j}{k}} &= 0, \qquad &\forall j,k,u \\ \label{eq:indepIncrements}
				\Cov{\eincrement{u}{j}{k}}{\eincrement{u'}{j'}{k'}} &= \kronecker{j}{j'}\kronecker{k}{k'}\kronecker{u}{u'}, \qquad &\forall j,j',k,k',u,u' 
			\end{align}

		\item $\wavelet{j}{k}{t} = \psi_{j,k-t}$ is a scalar representing a non-decimated wavelet.
	\end{itemize}

We can define the \emph{Cross-Evoluationary Wavelet Spectrum} $(N \times N)$ matrix : $\bm{S}_{j}(z) = \bm{W}_{j}(z)\bm{W}_{j}(z)'$.
This gives us the ability to express the \emph{local autocovariance} : $c^{(u,u')}(z,\tau) = \sum_{j=-\infty}^{-1} S_{j}^{(u,u')}(z) \ACwavelet{j}{\tau}$ where $\ACwavelet{j}{\tau} = \sum_{k} \wavelet{j}{k}{0}\wavelet{j}{k}{\tau}$, the \emph{autocorrelation wavelet}. The latter also define the \emph{inner product matrix of discrete autocorrelation wavelets} : $A_{jl} = \sum_{\tau}\ACwavelet{j}{\tau}\ACwavelet{l}{\tau}$, $A = \left\{ A_{jl}\right\}_{j,l \in \mathbb{N}}$ and its inverse : $\bar{A} = A^{-1}$.\\

Each $(m,n)$-element of the Cross-Evolutionary Wavelet Spectrum can be expressed as 
\begin{align*}
	\eCEWS{m}{n}{j}{k} = \sum_{u=1}^{N} \eW{m}{u}{j}{k}\eW{u}{n}{j}{k}, \quad \forall j,k
\end{align*}
From this definition it is not difficult to extend the CEWS to take into account the dependence structure between different scales and throught time : 
\begin{align}\label{eq:serial-scale-CEWS}
	S_{j,j'}^{m,n}\left( k/T, k'/T \right) = \sum_{u=1}^{N} \eW{m}{u}{j}{k}\eW{u}{n}{j'}{k'}, \quad \forall j,j',k,k'
\end{align}
We make the following assumption regarding the latter object, 
\begin{align}\label{eq:scale-indep-spectrum}
S_{j,j'}^{(m,n)}\left( \textcolor{blue}{k/T}, \textcolor{blue}{k/T} \right) = 
	\begin{cases}
		\eCEWS{m}{n}{j}{k} & \text{ if } j=j' \\
		0 & \text{ otherwise}
	\end{cases}
\end{align}
This assumption \textcolor{orange}{(possible improvement : condition similar to Chamberlain ?)}  imposes no dependence between different scales of decomposition. Notice that we don't restrict the serial dependence. 

\subsection{Estimation of MvLSW}
		\begin{itemize}
			\item $\E{\rawPeriodo{j}{k}} = \sum_{l=-J}^{-1} A_{jl}\CEWS{l}{k} + O(T^{-1})$ \hspace{2cm} (biaised estimator)
		\end{itemize}

\subsection{Estimation of MvLSW}
\begin{align}
	\coeffs{j}{k} &= \sum_{t=0}^{T-1}\bm{X}_t \wavelet{j}{k}{t} &\quad \text{ (empirical wavelet coefficients) } \label{eq:empiricalWaveletCoeffs}\\
	\rawPeriodo{j}{k} &= \coeffs{j}{k}\coeffs{j}{k}'  &\quad \text{ (raw wavelet periodogram) } \label{eq:rawWaveletPeriodo}\\
	\correctedPeriodo{j}{k} &= \sum_{l=-J}^{-1} \bar{A}_{j,l}\rawPeriodo{l}{k} &\quad \text{ (corrected periodogram) } \label{eq:correctedPeriodo} \\
	\smoothPeriodo{j}{k} &= \frac{\displaystyle 1}{\displaystyle 2M+1} \sum_{m=-M}^{M}\rawPeriodo{j}{k+m}  &\quad \text{ (smooth periodogram) } \label{eq:smoothPeriodo} \\
	\estimCEWS{j}{k} &= \sum_{l=-J}^{-1}\bar{A}_{jl}\smoothPeriodo{l}{k} \nonumber \\
			        &= \frac{\displaystyle 1}{\displaystyle 2M+1} \sum_{m=-M}^{M}\correctedPeriodo{j}{k+m} \nonumber \\
			        &= \frac{\displaystyle 1}{\displaystyle 2M+1} \sum_{m=-M}^{M}\sum_{l=-J}^{-1} \bar{A}_{j,l}\rawPeriodo{l}{k+m} &\quad \text{ (final estimator of CEWS) } \label{eq:finalEstimator}
\end{align}

\subsection{Notes}
	\begin{itemize}
		\item The dependence structure is entirely in $\bm{W}_{j}(z)$, not in $\increment{j}{k}$.
		\item The lower-triangular form of $\bm{W}_{j}(z)$ allows us to use the Cholesky decomposition on $\bm{S}_{j}(z)$.
	\end{itemize}

\section{Factor Model}
	\begin{itemize}
		\item The factor structure is imposed on the following : 
			\begin{align} \label{eq:factorStructure}
				\W{j}{k} \increment{j}{k} = \loadings{j}{k} \factors{k} + \idioError{j}{k}
			\end{align}
			, not only on $\increment{j}{k}$ since they are assumed orthonormal.
		\item Assumptions : 	
			\begin{enumerate}
				\item $\factors{k} \sim (\bm{0}, \bm{\Sigma}_F)$, where $\bm{\Sigma}_F$ is a diagonal positive definite $(K \times K)$ matrix. 
				\item $\factors{k} \perp \idioError{j}{k'}$, $\forall j,k,k'$
				\item $\idioError{j}{k} \sim (\bm{0}, \bm{\Sigma}_{\epsilon})$, where $\bm{\Sigma}_{\epsilon}$ has bounded eigenvalues. \textcolor{blue}{Note : make $\bm{\Sigma}_{\epsilon}$ dependent on time ? what about serial dependence ?}
				\item  $\loadings{j}{k}'\loadings{l}{m} = \bm{0}, \forall j \neq l, \forall k \neq m$. 
			\end{enumerate}
		\item We can then represent the CEWS with the factor structure : 
		\begin{align*}
			\Var{\W{j}{k}\increment{j}{k}}        	    &= \Var{\loadings{j}{k}\factors{k}} + \Var{\idioError{j}{k}} \\
			\W{j}{k} \Var{\increment{j}{k}} \W{j}{k}' &= \loadings{j}{k}\Var{\factors{k}}\loadings{j}{k}' +  \Var{\idioError{j}{k}}\\
			\W{j}{k}\W{j}{k}' &=  \loadings{j}{k}\bm{\Sigma}_{F}\loadings{j}{k}' +  \bm{\Sigma}_{\epsilon} &\text{ from } \eqref{eq:indepIncrements}\\
			\CEWS{j}{k} &= \loadings{j}{k}\bm{\Sigma}_{F}\loadings{j}{k}' +  \bm{\Sigma}_{\epsilon}
		\end{align*}
	\end{itemize}


%	An important quantity to analyse is : 
%	\begin{align*}
%		\W{j}{k}\increment{j}{k}\increment{j'}{k'}'\W{j'}{k'}' = \loadings{j}{k}\factors{k}\factors{k'}'\loadings{j'}{k'}' + \idioError{j}{k}\factors{k'}'\loadings{j'}{k'}' + \loadings{j}{k}\factors{k}\idioError{j'}{k'}' + \idioError{j}{k}\idioError{j'}{k'}'
%	\end{align*}
%	Each $(m,n)$-element of the matrix on the RHS can be written as : 
%	\begin{align*}
%		\sum_{u}\sum_{u'} \eW{m}{u}{j}{k}\eincrement{u}{j}{k}\eincrement{u'}{j'}{k'}\eW{u'}{n}{j'}{k'} \left(= \sum_{u}\ \eW{m}{u}{j}{k}\eincrement{u}{j}{k} \sum_{u'}\eW{u'}{n}{j'}{k'} \eincrement{u'}{j'}{k'}\right)
%	\end{align*}
%	\footnote{Can analyse this term as $\sum_u  \eW{m}{u}{j}{k}\eincrement{u}{j}{k} \sum_{u'} \eW{u'}{n}{j'}{k'}\eincrement{u'}{j'}{k'}$ and then use Slutsky ?}.
%
%	The expectation of each term is therefore, 
%	\begin{align*}
%		\E{\eW{m}{u}{j}{k}\eincrement{u}{j}{k}\eincrement{u'}{j'}{k'}\eW{u'}{n}{j'}{k'}} &= \eW{m}{u}{j}{k}\E{\eincrement{u}{j}{k}\eincrement{u'}{j'}{k'}}\eW{u'}{n}{j'}{k'}\\
%		&= \begin{cases}
%			\eW{m}{u}{j}{k}\eW{u}{n}{j}{k} \quad &\text{ if } j=j',k=k',u=u'\\
%			0  \quad & \text{otherwise}
%		      \end{cases} \quad \text{form } \eqref{eq:indepIncrements}
%	\end{align*}
%	To apply a WLLN we need finite variances on each term :  
%	\begin{align*}
%		&\Var{\eW{m}{u}{j}{k}\eincrement{u}{j}{k}\eincrement{u'}{j'}{k'}\eW{u'}{n}{j'}{k'}} = \left(\eW{m}{u}{j}{k}\right)^{2}\Var{\eincrement{u}{j}{k}\eincrement{u'}{j'}{k'}}\left(\eW{u'}{n}{j'}{k'}\right)^{2}\\
%		&\Var{\eincrement{u}{j}{k}\eincrement{u'}{j'}{k'}} = 
%			\begin{cases}
%				\E{\left(\eincrement{u}{j}{k}\eincrement{u'}{j'}{k'}\right)^{2}} = \footnotesize \begin{cases}
%															\E{(\eincrement{u}{j}{k})^{2}}\E{(\eincrement{u'}{j'}{k'})^{2}} = 1 &\text{ if } \eincrement{u}{j}{k} \text{ is Gaussian and from}\eqref{eq:indepIncrements}\\ 			
%															\leq \E{(\eincrement{u}{j}{k})^{4}}^{1/2}\E{(\eincrement{u'}{j'}{k'})^{4}}^{1/2}	&\text{ otherwise and from Cauchy. }			
%														\end{cases} &\text{ if } j\neq j', k\neq k', u\neq u' \normalsize\\
%				\Var{(\eincrement{u}{j}{k})^{2}} \leq \E{(\eincrement{u}{j}{k})^{4}} & \text{ otherwise }
%			\end{cases}
%	\end{align*}
%	which suggests that we need finite fourth moment for the increment of the LSW process in order to have convergence in probability. \\
%	If $\E{(\eincrement{u}{j}{k})^{4}}< \infty$, \textcolor{blue}{convergence ? But double sum...}.\\
%	
%	\textcolor{blue}{
%	Each $(m,n)$-element of the matrix on the RHS can be written as : 
%	\begin{align*}
%		\sum_{u}\sum_{u'} \eW{m}{u}{j}{k}\eincrement{u}{j}{k}\eincrement{u'}{j'}{k'}\eW{u'}{n}{j'}{k'} = \sum_{u}\ \eW{m}{u}{j}{k}\eincrement{u}{j}{k} \sum_{u'}\eW{u'}{n}{j'}{k'} \eincrement{u'}{j'}{k'}
%	\end{align*}
%	We can analyse the convergence of the two sum independently and then apply Slutsky. 
%	The expectation of each term in the first sum is therefore, 
%	\begin{align*}
%		\E{\eW{m}{u}{j}{k}\eincrement{u}{j}{k}} &= \eW{m}{u}{j}{k}\E{\eincrement{u}{j}{k}} = 0\\
%	\end{align*}
%	To apply a WLLN we need finite variances on each term :  
%	\begin{align*}
%		\Var{\eW{m}{u}{j}{k}\eincrement{u}{j}{k}} &= \left(\eW{m}{u}{j}{k}\right)^{2}\Var{\eincrement{u}{j}{k}}\\
%								        &= \left(\eW{m}{u}{j}{k}\right)^{2} < \infty \quad &\text{from }\eqref{eq:indepIncrements} \text{ and }\eqref{eq:finiteEnergy}
%	\end{align*}
%	Then, 
%	\begin{align*}
%		\frac{1}{N} \sum_{u=1}^{N} \eW{m}{u}{j}{k}\eincrement{u}{j}{k} \overset{p}{\longrightarrow} 0 \quad \text{when } N \longrightarrow \infty
%	\end{align*}
%	Finally by Slutsky, 
%	\begin{align*}
%		\frac{1}{N}\sum_{u}\ \eW{m}{u}{j}{k}\eincrement{u}{j}{k} \frac{1}{N}\sum_{u'}\eW{u'}{n}{j'}{k'} \eincrement{u'}{j'}{k'} \overset{p}{\longrightarrow} 0
%	\end{align*}
%	\textcolor{blue}{There is a problem... This result would mean that the factor structure is imposed on a null matrix asymptotically. }}





\end{document}


% https://mathoverflow.net/questions/207452/eigenvectors-as-continuous-functions-of-matrix-diagonal-perturbations





























	
\end{document}